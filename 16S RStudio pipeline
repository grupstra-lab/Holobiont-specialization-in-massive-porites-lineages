#This code conducts the cleaning and analyses of 16S reads for this project
$Some of the code here was adapted from Colleen Bove: https://github.com/seabove7

library("dada2")
library("phyloseq")
library("decontam")
library("plotrix")
library("car")
library("performance")

#Set working directory

#Set path to 16S files
path_16S <- "~/Documents/Github/Palau/plobmicrobe/concat_16s_dl"
fnFs <- sort(list.files(path_16S, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path_16S, pattern = "_R2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample_names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

sample_names2 <-as.data.frame(sample_names)
meta <- read.csv("samplemap.csv") 

# reordering these so they match the order of sample names 
meta <- meta[match(sample_names, meta$Sample),] 
rownames(meta) <- meta$Sample

#### check for primers ####
FWD <- "GTGYCAGCMGCCGCGGTAA"  ## CHANGE ME to your forward primer sequence
REV <- "GGACTACNVGGGTWTCTAAT"  ## CHANGE ME...

allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}

FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

## create list of forward/reverse samples from filters
fnFs_filtN <- file.path(path_16S, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs_filtN <- file.path(path_16S, "filtN", basename(fnRs))

## filter/trim the samples
filterAndTrim(fnFs, fnFs_filtN, fnRs, fnRs_filtN, maxN = 0, multithread = TRUE)

#primerHits function
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}

#towards the beginning
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs_filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs_filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs_filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs_filtN[[1]]))

#at the end
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs_filtN[[4]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs_filtN[[4]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs_filtN[[4]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs_filtN[[4]]))

## Install cutadapt to local computer and set path here (python3 -m pip install --user --upgrade cutadapt)
cutadapt = "/Users/casgrupstra/opt/miniconda3/envs/cutadaptenv/bin/cutadapt"
system2(cutadapt, args = "--version")

## look for cutadapt path and create one if it doesn't exist
path_cut <- file.path(path_16S, "cutadapt")
if(!dir.exists(path_cut)) dir.create(path_cut)

## create list of forward/reverse samples to put the cutadapt samples
fnFs_cut <- file.path(path_cut, basename(fnFs)) # 16S path with the subdirectory 'cutadapt' then the sample names
fnRs_cut <- file.path(path_cut, basename(fnRs))

# forward/reverse complements of primers
FWD_RC <- dada2:::rc(FWD)
REV_RC <- dada2:::rc(REV)
R1_flags <- paste("-g", FWD, "-a", REV_RC) # Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R2_flags <- paste("-G", REV, "-A", FWD_RC) # Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1_flags, R2_flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs_cut[i], "-p", fnRs_cut[i], # output files
                             fnFs_filtN[i], fnRs_filtN[i])) # input files
}

#towards the beginning
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs_cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs_cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs_cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs_cut[[1]]))
#at the end
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs_cut[[4]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs_cut[[4]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs_cut[[4]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs_cut[[4]]))

#No more primers were detected

### View example quality plots
#Viewing the quality of 2 of the forward samples
fnFs_cut <- file.path(path_16S, "cutadapt", paste0(sample_names, "_16S_R1.fastq")) # 16S path with the subdirectory 'cutadapt' then the sample names
fnRs_cut <- file.path(path_16S, "cutadapt", paste0(sample_names, "_16S_R2.fastq"))

## Plot samples:
str(fnFs_cut)
plotQualityProfile(fnFs_cut[1:55])

## Plot reverse samples:
plotQualityProfile(fnRs_cut[1:55])

### Perform the filtering

#You can see in above figures that the quality of reads drop off towards the end, so we need to filter out these low quality reads

# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path_16S, "filtered", paste0(sample_names, "_filt_F.fastq.gz"))
filtRs <- file.path(path_16S, "filtered", paste0(sample_names, "_filt_R.fastq.gz"))

# name the file paths with the corresponding sample names
names(filtFs) <- sample_names
names(filtRs) <- sample_names
filt_out <- filterAndTrim(fnFs_cut, filtFs, fnRs_cut, filtRs,
                          truncLen = c(220, 210), # cut for the forward and reverse files
                          maxN = 0, # DADA2 requires no Ns
                          maxEE = c(1, 1), # reads with higher than maxEE "expected errors" will be discarded
                          truncQ = 2, # Truncate reads at the first instance of a quality score less than or equal to truncQ
                          rm.phix = TRUE, # If TRUE, discard reads that match against the phiX genome (kinda control bacteria)
                          compress = TRUE, #  Whether the output fastq file should be gzip com- pressed.
                          multithread = TRUE) # On Windows set multithread = FALSE
head(filt_out)
# the percent of reads making it through the filter and trim step:
filter_perc <- (colSums(filt_out)[2]/colSums(filt_out)[1] * 100) # 93.9%

######### Check the filtering

#Viewing the quality of the same 2 of the forward samples post filtering and trimming
## plot the filtered forward samples
plotQualityProfile(filtFs[1:55])
plotQualityProfile(filtRs[1:55])

## DADA2 sample processing {.tabset}
### Error Rates

### Learning the error rates takes a few minutes so I am saving it as an R object to call in line. If things are modified upstream, this needs to be rerun.
errF <- learnErrors(filtFs, multithread = TRUE) #103366120 total bases in 469846 reads from 27 samples will be used for learning the error rates.
errR <- learnErrors(filtRs, multithread = TRUE) #102509610 total bases in 488141 reads from 28 samples will be used for learning the error rates.

# visualize errors
?plotErrors()
errF_plot <- plotErrors(errF, nominalQ = TRUE)
errR_plot <- plotErrors(errR, nominalQ = TRUE)
## Save the error rates
#The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. We want the estimated error rates (black line) to be a good fit to the observed rates (points), and the error rates to drop with increased quality.

### Sample Inference

#We are now ready to apply [the core sample inference algorithm](https://www.nature.com/articles/nmeth.3869#methods) to the filtered and trimmed sequence data.

#Forwards
# Core sample inference of forward samples
dadaFs <- dada(filtFs, err = errF, multithread = TRUE)

#Inspecting the returned dada-class object for the first forward sample:
# Inspecting the returned dada-class object:
dadaFs[[1]]

#67 sequence variants were inferred from 3780 input unique sequences. Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16

#Reverses
dadaRs <- dada(filtRs, err = errR, multithread = TRUE)

#Inspecting the returned dada-class object for the first forward sample:
# Inspecting the returned dada-class object:
dadaRs[[1]]

#62 sequence variants were inferred from 3474 input unique sequences. Key parameters: OMEGA_A = 1e-40, OMEGA_C = 1e-40, BAND_SIZE = 16

######### Merge paired reads
#We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (but these conditions can be changed via function arguments).

mergers <- mergePairs(dadaF = dadaFs, # dada-class object(s) generated by denoising the forward reads.
                      derepF = filtFs, # derep-class object(s) used as input to the the dada function when denoising (for)
                      dadaR = dadaRs, # dada-class object(s) generated by denoising the reverse reads.
                      derepR = filtRs, # derep-class object(s) used as input to the the dada function when denoising (rev)
                      verbose = TRUE) # summary of the function results are printed to standard output (verbose = TRUE)
# Inspect the merger data.frame from the first sample
#head(mergers[[1]])

#The mergers object is a list of data.frames from each sample. Each data.frame contains the merged sequence, its abundance, and the indices of the forward and reverse sequence variants that were merged. Paired reads that did not exactly overlap were removed by mergePairs, further reducing spurious output.
 
### Construct sequence table
#We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

seqtab <- makeSequenceTable(mergers)
#dim(seqtab) # 38 samples, 3546 sequence variants
# Inspect distribution of read lengths
table(nchar(getSequences(seqtab))) 
hist(nchar(getSequences(seqtab)))

#The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. This table contains `r dim(seqtab)[2]` ASVs.

#After viewing the distribution of read lengths, it looks like we have some that fall outside the expected range (244 - 264) so we will go ahead and remove these non target length sequences.

# Remove any ASVs that are considerably off target length
seqtab_trimmed <- seqtab[,nchar(colnames(seqtab)) %in% seq(251,256)]
table(nchar(getSequences(seqtab_trimmed)))

#251  252  253  254  255  256 
#12   95 4684  257   14    9 


######## Remove chimeras
#The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

seqtab_nochim <- removeBimeraDenovo(seqtab_trimmed, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab_nochim)

rownames(seqtab_nochim) <- gsub("_filt_F.fastq.gz", "", rownames(seqtab_nochim))
perc_nochim <- (sum(seqtab_nochim)/sum(seqtab) * 100) # percent non chimera sequences kept = 99.46572
perc_nochim
## Save the chimera-free ASV table to load in downstream analyses
save(seqtab_nochim, file = "seqtab_nochim.Rdata")

#Track how many samples made it through the pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(filt_out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab_nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample_names
track
write.csv(track, "Track_Reads.csv", row.names = FALSE)


# Assign taxonomy

#It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides
#a native implementation of the naive Bayesian classifier method for this purpose. The `assignTaxonomy` function takes as input a set of sequences 
#to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least 
#`minBoot` bootstrap confidence.

#The dada2 package GitHub maintains the most updated versions of the [Silva databases.](https://benjjneb.github.io/dada2/training.html), but 
#I downloaded the databases from the associated [Zenodo](https://zenodo.org/record/4587955#.YuBdCS-cbOQ). The versions in this GitHub repository, 
#used here, were last updated on 26 July 2022.

## going to move these out of this repo because they are too big:
# SCC path to SILVA files: /projectnb/davieslab/bove/SILVA_files
# local path to SILVA files: /Users/colleen/Dropbox/BU/DaviesLab/ACER_microplastics/SILVA/

silva_path <- "~/Documents/Github/Palau/plobmicrobe/silva_nr99_v138.1_train_set.fa"
taxa <- assignTaxonomy(seqtab_nochim, silva_path, multithread=TRUE)

# Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between
# ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only
# appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for
# the Silva and RDP 16S databases. To follow the optional species addition step, download the
# silva_species_assignment_v138.1.fa.gz file, and place it in the directory with the fastq files.

silva_sp_path <- "~/Documents/Github/Palau/plobmicrobe/silva_species_assignment_v138.1.fa"
taxa_sp <- addSpecies(taxa, silva_sp_path)

save(taxa, taxa_sp, file = "taxa_assign.Rdata")

##Inspect taxa assignments
load("taxa_assign.Rdata")
taxa_print <- taxa_sp # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
head(taxa_print)

# Export sequence table with genus and species assignments as phyloseq objects
phylo <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = FALSE),
                  sample_data(meta),
                  tax_table(taxa))

phylo_sp <- phyloseq(otu_table(seqtab_nochim, taxa_are_rows = FALSE),
                     sample_data(meta),
                     tax_table(taxa_sp))


# originally, sequences were the taxa names
seqs_ps <- taxa_names(phylo_sp)
# add a sequence column to matrix
taxa2 <- cbind(tax_table(phylo_sp), seqs_ps)
rownames(taxa2) <- paste0("ASV", seq(ntaxa(phylo_sp))) # rename these rows
# change taxa names to ASVs in the phyloseq object
taxa_names(phylo_sp) <- paste0("ASV", seq(ntaxa(phylo_sp)))
# save a file of the taxa table with ASVs and sequences
taxtab_seqs <- cbind(data.frame(tax_table(phylo_sp)), seqs_ps)
write.csv(taxtab_seqs, "taxatable_withsequences.csv")
## Save as RDS objects
save(phylo, phylo_sp, taxa2, file = "phylo_objs.Rdata") # save both with and without species level and ASV labeling

#####                    #####
##### R  A N A L Y S I S #####
#####                    #####

# Load the phyloseq objects created above
load(file = "phylo_objs.Rdata")
?subset_taxa()
# remove all mitochondria from family  
ps_nomito <- subset_taxa(phylo_sp, (Family != "Mitochondria") | is.na(Family))
# remove all chloroplast from order
ps_nochlor <- subset_taxa(ps_nomito, (Order != "Chloroplast") | is.na(Order))
# remove all non-bacteria
ps_clean <- subset_taxa(ps_nochlor, (Kingdom == "Bacteria"))
ps_clean
ps_clean1 <- ps_clean

## total number of contam above removed
contam_tot <- dim(phylo_sp@otu_table)[[2]] - dim(ps_clean@otu_table)[[2]] #480

### Remove neg control contamination

## Visualize library sizes per sample
df <- data.frame(sample_data(ps_clean)) # make sample_data a dataframe
df$LibrarySize <- sample_sums(ps_clean) # add reads per sample
df <- df[order(df$LibrarySize),] # reorder df from lowest to highest library size
df$Index <- seq(nrow(df)) # add index for plotting
# plot library size by treatment
ggplot(data = df, aes(x = Index, y = LibrarySize, color = as.factor(Lineage))) + 
  geom_point() +
  ggtitle("Library size by treatment of all samples")

## identify contaminants based on neg controls
sample_data(ps_clean)$is.neg <- sample_data(ps_clean)$Species == "NC" # identify neg controls & add to sample data
contamdf_freq <- isContaminant(ps_clean, neg = "is.neg", threshold = 0.5) # The frequency of each sequence (or OTU) in the input feature table as a function of the concentration of amplified DNA in each sample is used to identify contaminant sequences.

neg_contam <- table(contamdf_freq$contaminant)
head(which(contamdf_freq$contaminant)) # which ASVs are being identified as contaminants 


# Make phyloseq object of presence-absence in negative controls and true samples
ps_pa <- transform_sample_counts(ps_clean, function(abund) 1 * (abund > 0)) # transforms the sample counts of a taxa abundance matrix
ps_pa_neg <- prune_samples(sample_data(ps_pa)$Species == "NC", ps_pa) # filters samples for controls
ps_pa_pos <- prune_samples(sample_data(ps_pa)$Species != "NC", ps_pa) # filters to remove controls
# Make dataframe of prevalence in positive and negative samples
df_pa <- data.frame(pa_pos = taxa_sums(ps_pa_pos),
                    pa_neg = taxa_sums(ps_pa_neg),
                    contaminant = contamdf_freq$contaminant)
# plot prevalence of neg controls against true samples
ggplot(data = df_pa, aes(x = pa_neg, y = pa_pos, color = contaminant)) +
  geom_point() +
  xlab("Prevalence (Negative Controls)") +
  ylab("Prevalence (True Samples)")

# remove contam taxa from ps_clean:
ps_clean1 <- prune_taxa(!contamdf_freq$contaminant, ps_clean)

# also remove negative controls, don't need them anymore I think
ps_cleaner <- subset_samples(ps_clean1, (Species != "NC"))
ps_cleaner

ps <- prune_samples(sample_sums(ps_cleaner)>=2000, ps_cleaner) #if there are obvious low outliers, remove (we're removing any sequence with less than 2000 reads)
sample_data(ps)

#Save new phyloseq object
#saveRDS(ps,file="phyloseq_cleaned.rds")


### DONE CLEANING AND LOAD CLEANED OBJECT FOR ANALYSIS ###
##########################################################

#ps<-readRDS("phyloseq_cleaned.rds")



